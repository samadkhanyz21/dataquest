{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNNs) are a particular type of neural network architecture that's particularly well suited for working with images / computer vision tasks. \n",
    "\n",
    "What are some use cases / examples of computer vision you come across daily or have heard of?\n",
    "+ object recognition, maybe implemented on a robot\n",
    "+ facial recognition\n",
    "+ self driving car / very useful for parking if you are not that great in do that\n",
    "+ brand recognition\n",
    "+ detecting cancer from Xrays/tomography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend some time looking at [this explanation of Image Kernels](https://setosa.io/ev/image-kernels/) (feel free to play around with all the settings!) and discuss the following questions:\n",
    " - What does the number at each pixel location in a greyscale image mean?\n",
    " - What is a kernel?\n",
    " - How is a kernel applied to an image?\n",
    " - How does changing the numbers in the kernel affect the output image?\n",
    " - What features of an image can we highlight using a kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine we have this very simple 5x5 image of a face. Do you see the eyes, the nose and the big smile?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='input_image.jpeg' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Kernel/filter_ is one of the main concepts in CNNs. Let's for example take this 3x3 image to be out filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='filter.jpeg' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We _convolve_ the filter with our image by sliding the 3x3 filter image over the 5x5 image and in each position we multiply the overlapping \"pixels\", sum them up and take the sum as a value of the resulting image in that position. We then move the filter one cell to the right and repeat the process. When we're done with the first row, we move to the second row, etc... Let's calculate a couple of values together and then you'll do the rest on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= 'Convolution_operation.jpeg' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='cnn.gif' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now continue on your own until you have covered your whole original image with your filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='feature_map.jpeg' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<img src='vertical_edge_example.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the resulting image:\n",
    "* convolved image has smaller dimensionality\n",
    "* extracted the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What is the size of the resulting image, if the size of the original image is `n x n` and the size of the filter is `f x f`?\n",
    "\n",
    "**A**: `n-f+1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just ran your first edge-detection algorithm! \n",
    "\n",
    "That is basically how convolutional neural networks work in very simple terms.\n",
    "\n",
    "It is important to understand that in the CNN, the kernels in the layers are determined during training using backpropagation. We do not tell the network what they are, it learns on its own! This can also mean that some features that it detects are not really interpretable by humans.\n",
    "\n",
    "So, we don't specify the filters for them to use, filters are the parameters (weights) that they learn (your `w`s).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a couple of other concepts that are important for CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing a convolution on an image, we end up losing the pixels on the edge. In many cases, this is not a huge issue, as we expect the important information (e.g. the face or the object we are trying to detect and classify) to be somewhere nearer the middle. Also smaller images mean fewer parameters for the network, ie. less computational cost.\n",
    "\n",
    "However if we want to keep the outside pixels after all, we can use **padding**:\n",
    " - we add a single layer of pixels around the outside of the original image in order to extend it.\n",
    " - this new, slightly larger image is now convolved with the kernel\n",
    " - **the output is the same size as the original image**\n",
    " \n",
    "The two most common kinds of padding are:\n",
    " - **same** - the fake new pixels are set to zero (it preserves image size)\n",
    " \n",
    "\n",
    "A term often seen in the context of CNNs is:\n",
    " - **no padding** - `padding = valid`\n",
    "\n",
    "No padding is called **valid** because only valid pixels (i.e. those present in the original image) are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![same_padding_no_strides.gif](same_padding_no_strides.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What is the size of the resulting image, if the size of the original image is `n x n`, the size of the filter is `f x f`, and we use `p` pixels of padding on each side?\n",
    "\n",
    "**A**: `n-f+1+2p`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stride value/step controls how the filter moves around and covers the original image. Sometimes you may want to \"skip\" around the original image and not slide pixel by pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![padding_strides.gif](padding_strides.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(Source)](https://github.com/vdumoulin/conv_arithmetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What is the size of the resulting image, if the size of the original image is `n x n`, the size of the filter is `f x f`, we use padding of `p` and stride of `f`?\n",
    "\n",
    "**A**: `(n-f+2p)/s + 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color (RGB) image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a color (RGB) image we start with three filters of the same type corresponding to each color channel, perform convolution like before, and then sum up three channels to form the resulting image that is now 4x4x1. \n",
    "\n",
    "(For filters of the same type we mean that they are specialized in doing the same task, e.g, in extracting horizontal edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One layer of CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rgb.jpeg](rgb.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CNNs we generally use more than one filter in each layer, so the third dimension of the resulting image will be equal to the number of filters used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rgb_2.png](rgb_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the operation of convolution, we add a different bias to each obtained feature map and then apply on them an activation function. \n",
    "\n",
    "How to calculate the number of parameters to be learned in a Convolutional layer?\n",
    "\n",
    "$n_{parameters} = (f*f*n_{channels}) * n_{different/filter} + n_{bias}$\n",
    "\n",
    "where:\n",
    "+ $n_{bias} =  n_{different/filter}$\n",
    "+ $n_{channels}$ corresponds to the number of channels of the convolved image/feature map \n",
    "\n",
    "So for the above example \n",
    "$(3*3*3)*2 + 2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diagrams of CNNS, this is usually represented in this way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one_layer.png](one_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(Source)](http://datahacker.rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our feature space even smaller, we can reduce the convolved images further by **pooling**, where we take some numeric representation for subsets of the convolved image. This reduces the resolution of the image, and in fact makes it more robust against slight differences in locations of features. Pooling is also known as **subsampling**.\n",
    "\n",
    "In the examples below we have:\n",
    "- **max pooling** where we take 2x2 windows ($f=2$) of our convolved image, take the **maximum value of each** and **return an even smaller image**. We also have a **stride** of 2 ($s=2$), i.e. we don't have overlapping subsections.\n",
    "    - max pooling ends up highlighting the most strongly present features in the sections of the image \n",
    "- **average pooling** where we take the **mean value** of each 2x2 window.\n",
    "    - average pooling shows the presence of a feature on average in a section of the image\n",
    "\n",
    "\n",
    "In addition to convolutional layers that we saw above, in CNN we also have _pooling layers_. Pooling accumulates/pools together the features created in convolutional layers.\n",
    "\n",
    "Max pooling, the most common pooling method, selects only the highest value in a given pooling window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![max_pooling.png](max_pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull (pool, heh) everything we've learned and look at an example convolutional neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical architecture of CNNs has a couple of convolutional layers, followed by _fully connected_ layer, i.e. feed forward NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some of the most famous CNN architectures it is common to alternate convolutional and pooling layers, like this. Shown below is one of the classical CNN architecture, LeNet-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='cnn_with_poolin.jpeg' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first layer of this last network only. What is the number of parameters we have here?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convolutional neural network:\n",
    "* `(5*5*3 +1)*6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of filters:\n",
    "* lowers the number of parameters\n",
    "* makes sense because a filter used for detecting a particular feature could be useful in multiple areas of an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixels close by are related:\n",
    "* flattening the image would lose some of this information\n",
    "* pixels far away aren't: we don't need to connect each of the pixels of the input with the each of the pixels of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Matplotlib requires numpy>=1.20; you have 1.19.5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepL/lib/python3.8/site-packages/matplotlib/__init__.py:229\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parse_version(module\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse_version(minver):\n\u001b[1;32m    225\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatplotlib requires \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m                               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 229\u001b[0m \u001b[43m_check_versions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# The decorator ensures this always returns the same handler (and it is only\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# attached once).\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache()\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_handler\u001b[39m():\n",
      "File \u001b[0;32m~/anaconda3/envs/deepL/lib/python3.8/site-packages/matplotlib/__init__.py:225\u001b[0m, in \u001b[0;36m_check_versions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(modname)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parse_version(module\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse_version(minver):\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatplotlib requires \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: Matplotlib requires numpy>=1.20; you have 1.19.5"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow.keras as tk\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist,cifar10\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, BatchNormalization,Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "(xtrain,ytrain),(xtest,ytest) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shapes:\n",
    "xtrain.shape, ytrain.shape, xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To feed the images in a CNN we need to reshape our X data to the format \n",
    "# (batch/sample, width, heigth, channels):\n",
    "# The mnist data are black and white images so we need just 1 color channel\n",
    "\n",
    "Xtrain = xtrain.reshape(60000, 28, 28,1)\n",
    "Xtest = xtest.reshape(10000, 28, 28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always keep checking shapes:\n",
    "print('Xtrain shape:', Xtrain.shape)\n",
    "print(Xtrain.shape[0], 'train samples')\n",
    "print(Xtest.shape[0], 'test samples')\n",
    "print(Xtrain[0].shape, 'image shape')\n",
    "print('ytrain shape:', ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the train images  with the labels\n",
    "plt.figure(figsize=(16,16))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1,title=f'Number: {ytrain[i]}')\n",
    "    plt.imshow(xtrain[i],cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels are 10 numbers (0...9) we transform it into 10 classes \n",
    "# ytrain, ytest from number to categorical/dummies\n",
    "\n",
    "ytrain_cat = to_categorical(ytrain)\n",
    "ytest_cat = to_categorical(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from categorical to number\n",
    "np.argmax(ytrain_cat,axis=1),np.argmax(ytest_cat,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After preprocessing also y:\\n')\n",
    "print('Xtrain shape:', Xtrain.shape)\n",
    "print(Xtrain.shape[0], 'train samples')\n",
    "print(Xtest.shape[0], 'test samples')\n",
    "print(Xtrain[0].shape, 'image shape')\n",
    "print('ytrain cat shape:', ytrain_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeNet-5 in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement LeNet-5 architecture from above. \n",
    "\n",
    "Use `relu` activation function for convolutional and fully-connected (dense) layers, and `softmax` for the output layer.\n",
    "\n",
    "Q: How do we deal with the fact that our images (28x28) are smaller than the input images in LeNet-t (32, 32)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to layers documentation:\n",
    "+ [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/)\n",
    "+ [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/)\n",
    "+ [Flatten](https://keras.io/api/layers/reshaping_layers/flatten/)\n",
    "+ [Dense](https://keras.io/api/layers/core_layers/dense/)\n",
    "+ [Weight Initializers](https://keras.io/api/layers/initializers/)\n",
    "+ [Activation Functions](https://keras.io/api/layers/activations/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='cnn_with_poolin.jpeg' width=600 heigth=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement LeNet-5 architecture \n",
    "K.clear_session()\n",
    "model = Sequential([\n",
    "    \n",
    "    ### Convolutional layers\n",
    "    Conv2D(filters = ..., kernel_size = (...,...), \n",
    "           strides = (...,...), padding = ... , \n",
    "           activation = tk.activations... ,input_shape = (28,28,1)),  # use relu\n",
    "           \n",
    "    \n",
    "    MaxPooling2D(pool_size = (...,...), strides=(...,...), padding = ...),\n",
    "    \n",
    "    Conv2D(filters = ..., kernel_size = (...,...), \n",
    "           strides = (...,...), padding = ... , activation = tk.activations...) ,  # use relu\n",
    "            \n",
    "    \n",
    "    MaxPooling2D(pool_size = (...,...), strides=(...,...), padding = ...),\n",
    "    \n",
    "    ### Fully connected layers for classification\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(units=..., activation= tk.activations...),  # use relu\n",
    "    \n",
    "    Dense(units=..., activation=tk.activations...), # use relu\n",
    "          \n",
    "    \n",
    "    \n",
    "    # Output layer\n",
    "    Dense(units=..., activation=tk.activations...)\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the summary:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(optimizer=tf.optimizers.Adam(), # Adam\n",
    "              loss=tf.losses.categorical_crossentropy , # multiclass: 'categorical_crossentropy' \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(Xtrain, ytrain_cat, epochs=20,\n",
    "                    batch_size=60,\n",
    "                    validation_split = 0.2,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot loss and accuracy curves\n",
    "pd.DataFrame(data=history.history).plot()\n",
    "plt.grid(True)\n",
    "plt.xlabel('epochs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe in the loss and the accuracy curves? <br>\n",
    "Do you see overfitting? If this is the case, try to use some regularization\n",
    "+ [BatchNormalization](https://keras.io/api/layers/normalization_layers/batch_normalization/)\n",
    "+ [Dropout](https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "    \n",
    "\n",
    "In case you overcome the overfitting problem, you can now try to increase the hyperparameter epochs  in the `model.fit()`. Use the \n",
    "`EarlyStopping(monitor='val_loss', patience=5)` as a callback ([link](https://keras.io/api/callbacks/early_stopping/)).\n",
    " Remenber to set the hyperparameter callbacks in the `model.fit(...,callbacks=[callback])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**further ideas to examine the model**\n",
    "+ decode y_pred and ytest \n",
    "    + look above at the cell  `from categorical to number` \n",
    "+ calculate the confusion matrix and plot it with seaborn.heatmap() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Bonus if there is time) Visualisation of a CNN:\n",
    "\n",
    "There is an excellent visual explanation of how a CNN works [here](http://www.cs.cmu.edu/~aharley/nn_vis/cnn/2d.html).\n",
    "\n",
    "(Credit: A. W. Harley, \"An Interactive Node-Link Visualization of Convolutional Neural Networks,\" in ISVC, pages 867-877, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- [Nice visual explanation of convolutions on TDS](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)\n",
    "- [Another good guide to CNNs](https://towardsdatascience.com/beginners-guide-to-understanding-convolutional-neural-networks-ae9ed58bb17d)\n",
    "- [A very comprehensive paper on CNN arithmetic](https://arxiv.org/pdf/1603.07285.pdf)\n",
    "- [Great 15 minute video introduction to CNNs](https://www.youtube.com/watch?v=py5byOOHZM8)\n",
    "- [Explanation of pooling layers](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)\n",
    "- [Keras API documentation on Pooling Layers](https://keras.io/api/layers/pooling_layers/)\n",
    "- [GitHub Page for A. W. Harley's visualisation of a CNN](https://github.com/aharley/nn_vis)\n",
    "- [Good, in-depth paper about CNNs]((http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n",
    "- Some information and examples on [Batch Normalisation in CNNs](https://www.baeldung.com/cs/batch-normalization-cnn)\n",
    "- [Categorical Cross Entropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
